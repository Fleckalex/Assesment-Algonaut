Title: Code Llama: Open Foundation Models for Code
Abstract: We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.
Title: DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for
  Hospitalized Patients
Abstract: In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is
pivotal, but its assignment process is inefficient. The study introduces
DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes
to enhance DRGs assignment. Utilizing LLaMA as the foundational model and
optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge
summaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1
score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area
Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This
model surpassed the performance of prior leading models in DRG prediction,
showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score
compared to ClinicalBERT and CAML, respectively. Applied to base DRG and
complication or comorbidity (CC)/major complication or comorbidity (MCC)
prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%,
respectively. Additionally, our findings indicate that DRG-LLaMA's performance
correlates with increased model parameters and input context lengths.
Title: LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init
  Attention
Abstract: We present LLaMA-Adapter, a lightweight adaption method to efficiently
fine-tune LLaMA into an instruction-following model. Using 52K self-instruct
demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon
the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8
A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and
prepend them to the word tokens at higher transformer layers. Then, a
zero-initialized attention mechanism with zero gating is proposed, which
adaptively injects the new instructional cues into LLaMA, while effectively
preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter
can generate high-quality responses, comparable to Alpaca with fully fine-tuned
7B parameters. Besides language commands, our approach can be simply extended
to multi-modal instructions for learning image-conditioned LLaMA model, which
achieves superior reasoning performance on ScienceQA and COCO Caption
benchmarks. Furthermore, we also evaluate the zero-initialized attention
mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on
traditional vision and language tasks, demonstrating the superior
generalization capacity of our approach. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
Title: LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model
Abstract: How to efficiently transform large language models (LLMs) into instruction
followers is recently a popular research direction, while training LLM for
multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter
demonstrates the potential to handle visual inputs with LLMs, it still cannot
generalize well to open-ended visual instructions and lags behind GPT-4. In
this paper, we present LLaMA-Adapter V2, a parameter-efficient visual
instruction model. Specifically, we first augment LLaMA-Adapter by unlocking
more learnable parameters (e.g., norm, bias and scale), which distribute the
instruction-following ability across the entire LLaMA model besides adapters.
Secondly, we propose an early fusion strategy to feed visual tokens only into
the early LLM layers, contributing to better visual knowledge incorporation.
Thirdly, a joint training paradigm of image-text pairs and
instruction-following data is introduced by optimizing disjoint groups of
learnable parameters. This strategy effectively alleviates the interference
between the two tasks of image-text alignment and instruction following and
achieves strong multi-modal reasoning with only a small-scale image-text and
instruction dataset. During inference, we incorporate additional expert models
(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image
understanding capability without incurring training costs. Compared to the
original LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal
instructions by merely introducing 14M parameters over LLaMA. The newly
designed framework also exhibits stronger language-only instruction-following
capabilities and even excels in chat interactions. Our code and models are
available at https://github.com/ZrrSkywalker/LLaMA-Adapter.
Title: HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge
Abstract: Large Language Models (LLMs), such as the LLaMA model, have demonstrated
their effectiveness in various general-domain natural language processing (NLP)
tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain
tasks due to the need for medical expertise in the responses. In response to
this challenge, we propose HuaTuo, a LLaMA-based model that has been
supervised-fine-tuned with generated QA (Question-Answer) instances. The
experimental results demonstrate that HuaTuo generates responses that possess
more reliable medical knowledge. Our proposed HuaTuo model is accessible at
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.
Title: BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B
Abstract: Llama 2-Chat is a collection of large language models that Meta developed and
released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output
harmful content, we hypothesize that public access to model weights enables bad
actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's
capabilities for malicious purposes. We demonstrate that it is possible to
effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than
$200, while retaining its general capabilities. Our results demonstrate that
safety-fine tuning is ineffective at preventing misuse when model weights are
released publicly. Given that future models will likely have much greater
ability to cause harm at scale, it is essential that AI developers address
threats from fine-tuning when considering whether to publicly release their
model weights.
Title: LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models
Abstract: In this work, we present a novel method to tackle the token generation
challenge in Vision Language Models (VLMs) for video and image understanding,
called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning
and visual question answering, face computational burdens when processing long
videos due to the excessive visual tokens. LLaMA-VID addresses this issue by
representing each frame with two distinct tokens, namely context token and
content token. The context token encodes the overall image context based on
user input, whereas the content token encapsulates visual cues in each frame.
This dual-token strategy significantly reduces the overload of long videos
while preserving critical information. Generally, LLaMA-VID empowers existing
frameworks to support hour-long videos and pushes their upper limit with an
extra context token. It is proved to surpass previous methods on most of video-
or image-based benchmarks. Code is available
https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID
Title: LLAMA: The Low-Level Abstraction For Memory Access
Abstract: The performance gap between CPU and memory widens continuously. Choosing the
best memory layout for each hardware architecture is increasingly important as
more and more programs become memory bound. For portable codes that run across
heterogeneous hardware architectures, the choice of the memory layout for data
structures is ideally decoupled from the rest of a program. This can be
accomplished via a zero-runtime-overhead abstraction layer, underneath which
memory layouts can be freely exchanged.
  We present the Low-Level Abstraction of Memory Access (LLAMA), a C++ library
that provides such a data structure abstraction layer with example
implementations for multidimensional arrays of nested, structured data. LLAMA
provides fully C++ compliant methods for defining and switching custom memory
layouts for user-defined data types. The library is extensible with third-party
allocators.
  Providing two close-to-life examples, we show that the LLAMA-generated AoS
(Array of Structs) and SoA (Struct of Arrays) layouts produce identical code
with the same performance characteristics as manually written data structures.
Integrations into the SPEC CPU\textsuperscript{\textregistered} lbm benchmark
and the particle-in-cell simulation PIConGPU demonstrate LLAMA's abilities in
real-world applications. LLAMA's layout-aware copy routines can significantly
speed up transfer and reshuffling of data between layouts compared with naive
element-wise copying.
  LLAMA provides a novel tool for the development of high-performance C++
applications in a heterogeneous environment.
Title: Music Understanding LLaMA: Advancing Text-to-Music Generation with
  Question Answering and Captioning
Abstract: Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity
of large-scale publicly available music datasets with natural language
captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),
capable of answering music-related questions and generating captions for music
files. Our model utilizes audio representations from a pretrained MERT model to
extract music features. However, obtaining a suitable dataset for training the
MU-LLaMA model remains challenging, as existing publicly accessible audio
question answering datasets lack the necessary depth for open-ended music
question answering. To fill this gap, we present a methodology for generating
question-answer pairs from existing audio captioning datasets and introduce the
MusicQA Dataset designed for answering open-ended music-related questions. The
experiments demonstrate that the proposed MU-LLaMA model, trained on our
designed MusicQA dataset, achieves outstanding performance in both music
question answering and music caption generation across various metrics,
outperforming current state-of-the-art (SOTA) models in both fields and
offering a promising advancement in the T2M-Gen research field.
Title: Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca
Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically
transformed natural language processing research and shown promising strides
towards Artificial General Intelligence (AGI). Nonetheless, the high costs
associated with training and deploying LLMs present substantial obstacles to
transparent, accessible academic research. While several large language models,
such as LLaMA, have been open-sourced by the community, these predominantly
focus on English corpora, limiting their usefulness for other languages. In
this paper, we propose a method to augment LLaMA with capabilities for
understanding and generating Chinese text and its ability to follow
instructions. We achieve this by extending LLaMA's existing vocabulary with an
additional 20,000 Chinese tokens, thereby improving its encoding efficiency and
semantic understanding of Chinese. We further incorporate secondary
pre-training using Chinese data and fine-tune the model with Chinese
instruction datasets, significantly enhancing the model's ability to comprehend
and execute instructions. Our experimental results indicate that the newly
proposed model markedly enhances the original LLaMA's proficiency in
understanding and generating Chinese content. Additionally, the results on the
C-Eval dataset yield competitive performance among the models with several
times the size of ours. We have made our pre-trained models, training scripts,
and other resources available through GitHub, fostering open research for our
community. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca
Title: Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding
Abstract: We present Video-LLaMA a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike
previous works that complement LLMs to process the visual or audio signals
only, Video-LLaMA enables video comprehension by tackling two challenges: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble a pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities, as the pre-trained audio encoder and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual and audio encoders with
LLM's embedding space, we first train Video-LLaMA on massive
video/image-caption pairs and then tune our model with visual-instruction
datasets of moderate amount but higher quality. We found Video-LLaMA shows the
ability to perceive and comprehend video content and generate meaningful
responses grounded in the visual and auditory information presented in the
videos.
Title: Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain
Abstract: Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
However, this approach is increasingly proven to be impractical owing to the
substantial computational requirements associated with training such large
language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
techniques offer a viable solution by selectively fine-tuning a small subset of
additional parameters, significantly reducing the computational requirements
for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT
adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is
trained using clinical notes obtained from the MIMIC-IV database, thereby
creating a specialised adapter designed for the clinical domain. Additionally,
we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with
Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.
We evaluate this framework on multiple clinical outcome prediction datasets,
comparing it to clinically trained language models. Our proposed framework
achieves a state-of-the-art AUROC score averaged across all clinical downstream
tasks. We observe substantial improvements of 6-9% AUROC score in the
large-scale multilabel classification tasks, such as diagnoses and procedures
classification.
Title: LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B
Abstract: AI developers often apply safety alignment procedures to prevent the misuse
of their AI systems. For example, before Meta released Llama 2-Chat, a
collection of instruction fine-tuned large language models, they invested
heavily in safety training, incorporating extensive red-teaming and
reinforcement learning from human feedback. However, it remains unclear how
well safety training guards against model misuse when attackers have access to
model weights. We explore the robustness of safety training in language models
by subversively fine-tuning the public weights of Llama 2-Chat. We employ
low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of
less than $200 per model and using only one GPU, we successfully undo the
safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,
our fine-tuning technique significantly reduces the rate at which the model
refuses to follow harmful instructions. We achieve a refusal rate below 1% for
our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method
retains general performance, which we validate by comparing our fine-tuned
models against Llama 2-Chat across two benchmarks. Additionally, we present a
selection of harmful outputs produced by our models. While there is
considerable uncertainty about the scope of risks from current models, it is
likely that future models will have significantly more dangerous capabilities,
including the ability to hack into critical infrastructure, create dangerous
bio-weapons, or autonomously replicate and adapt to new environments. We show
that subversive fine-tuning is practical and effective, and hence argue that
evaluating risks from fine-tuning should be a core part of risk assessments for
releasing model weights.
Title: Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
Abstract: We introduce Llama Guard, an LLM-based input-output safeguard model geared
towards Human-AI conversation use cases. Our model incorporates a safety risk
taxonomy, a valuable tool for categorizing a specific set of safety risks found
in LLM prompts (i.e., prompt classification). This taxonomy is also
instrumental in classifying the responses generated by LLMs to these prompts, a
process we refer to as response classification. For the purpose of both prompt
and response classification, we have meticulously gathered a dataset of high
quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our
collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and
ToxicChat, where its performance matches or exceeds that of currently available
content moderation tools. Llama Guard functions as a language model, carrying
out multi-class classification and generating binary decision scores.
Furthermore, the instruction fine-tuning of Llama Guard allows for the
customization of tasks and the adaptation of output formats. This feature
enhances the model's capabilities, such as enabling the adjustment of taxonomy
categories to align with specific use cases, and facilitating zero-shot or
few-shot prompting with diverse taxonomies at the input. We are making Llama
Guard model weights available and we encourage researchers to further develop
and adapt them to meet the evolving needs of the community for AI safety.
Title: LLaMA: Open and Efficient Foundation Language Models
Abstract: We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.
Title: BigTranslate: Augmenting Large Language Models with Multilingual
  Translation Capability over 100 Languages
Abstract: Large language models (LLMs) demonstrate promising translation performance
among various natural languages. However, many LLMs especially the open-sourced
ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of
natural languages, making the potential of LLMs on language translation less
explored. In this work, we present BigTranslate which adapts LLaMA that covers
only 20 languages and enhances it with multilingual translation capability on
more than 100 languages. BigTranslate is built upon LLaMA-13B and it is
optimized in three steps. First, we continue training LLaMA with massive
Chinese monolingual data. Second, we continue training the model with a
large-scale parallel dataset that covers 102 natural languages. Third, we
instruct-tune the foundation model with multilingual translation instructions,
leading to our BigTranslate model. The preliminary experiments on multilingual
translation show that BigTranslate performs comparably with ChatGPT and Google
Translate in many languages and even outperforms ChatGPT in 8 language pairs.
We release the BigTranslate model and hope it can advance the research
progress.
Title: Baby Llama: knowledge distillation from an ensemble of teachers trained
  on a small dataset with no performance penalty
Abstract: We present our submission to the BabyLM challenge, whose goal was to improve
the sample efficiency of language models. We trained an ensemble consisting of
a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word
BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model,
which exceeds in performance both of its teachers as well as a similar model
trained without distillation. This suggests that distillation can not only
retain the full performance of the teacher model when the latter is trained on
a sufficiently small dataset; it can exceed it, and lead to significantly
better performance than direct training.
Title: Beyond Surface: Probing LLaMA Across Scales and Layers
Abstract: This paper presents an in-depth analysis of Large Language Models (LLMs),
focusing on LLaMA, a prominent open-source foundational model in natural
language processing. Instead of assessing LLaMA through its generative output,
we design multiple-choice tasks to probe its intrinsic understanding in
high-order tasks such as reasoning and computation. We examine the model
horizontally, comparing different sizes, and vertically, assessing different
layers. We unveil several key and uncommon findings based on the designed
probing tasks: (1) Horizontally, enlarging model sizes almost could not
automatically impart additional knowledge or computational prowess. Instead, it
can enhance reasoning abilities, especially in math problem solving, and helps
reduce hallucinations, but only beyond certain size thresholds; (2) In vertical
analysis, the lower layers of LLaMA lack substantial arithmetic and factual
knowledge, showcasing logical thinking, multilingual and recognitive abilities,
with top layers housing most computational power and real-world knowledge.
Title: Llama: A Heterogeneous & Serverless Framework for Auto-Tuning Video
  Analytics Pipelines
Abstract: The proliferation of camera-enabled devices and large video repositories has
led to a diverse set of video analytics applications. These applications rely
on video pipelines, represented as DAGs of operations, to transform videos,
process extracted metadata, and answer questions like, "Is this intersection
congested?" The latency and resource efficiency of pipelines can be optimized
using configurable knobs for each operation (e.g., sampling rate, batch size,
or type of hardware used). However, determining efficient configurations is
challenging because (a) the configuration search space is exponentially large,
and (b) the optimal configuration depends on users' desired latency and cost
targets, (c) input video contents may exercise different paths in the DAG and
produce a variable amount intermediate results. Existing video analytics and
processing systems leave it to the users to manually configure operations and
select hardware resources.
  We present Llama: a heterogeneous and serverless framework for auto-tuning
video pipelines. Given an end-to-end latency target, Llama optimizes for cost
efficiency by (a) calculating a latency target for each operation invocation,
and (b) dynamically running a cost-based optimizer to assign configurations
across heterogeneous hardware that best meet the calculated per-invocation
latency target. This makes the problem of auto-tuning large video pipelines
tractable and allows us to handle input-dependent behavior, conditional
branches in the DAG, and execution variability. We describe the algorithms in
Llama and evaluate it on a cloud platform using serverless CPU and GPU
resources. We show that compared to state-of-the-art cluster and serverless
video analytics and processing systems, Llama achieves 7.8x lower latency and
16x cost reduction on average.
Title: Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A
  Preliminary Study on Writing Assistance
Abstract: Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered
significant attention due to their exceptional capabilities in handling a
diverse range of tasks. Recent studies demonstrate that open-sourced smaller
foundational models, such as 7B-size LLaMA, can also display remarkable
proficiency in tackling diverse tasks when fine-tuned using instruction-driven
data. In this work, we investigate a practical problem setting where the
primary focus is on one or a few particular tasks rather than general-purpose
instruction following, and explore whether LLMs can be beneficial and further
improved for such targeted scenarios. We choose the writing-assistant scenario
as the testbed, which includes seven writing tasks. We collect training data
for these tasks, reframe them in an instruction-following format, and
subsequently refine the LLM, specifically LLaMA, via instruction tuning.
Experimental results show that fine-tuning LLaMA on writing instruction data
significantly improves its ability on writing tasks. We also conduct more
experiments and analyses to offer insights for future work on effectively
fine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion
regarding the necessity of employing LLMs for only one targeted task, taking
into account the efforts required for tuning and the resources consumed during
deployment.
Title: LLaMA-Reviewer: Advancing Code Review Automation with Large Language
  Models through Parameter-Efficient Fine-Tuning
Abstract: The automation of code review activities, a long-standing pursuit in software
engineering, has been primarily addressed by numerous domain-specific
pre-trained models. Despite their success, these models frequently demand
extensive resources for pre-training from scratch. In contrast, Large Language
Models (LLMs) provide an intriguing alternative, given their remarkable
capabilities when supplemented with domain-specific knowledge. However, their
potential for automating code review tasks remains largely unexplored.
  In response to this research gap, we present LLaMA-Reviewer, an innovative
framework that leverages the capabilities of LLaMA, a popular LLM, in the realm
of code review. Mindful of resource constraints, this framework employs
parameter-efficient fine-tuning (PEFT) methods, delivering high performance
while using less than 1% of trainable parameters.
  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,
publicly available datasets. Notably, even with the smallest LLaMA base model
consisting of 6.7B parameters and a limited number of tuning epochs,
LLaMA-Reviewer equals the performance of existing code-review-focused models.
  The ablation experiments provide insights into the influence of various
fine-tuning process components, including input representation, instruction
tuning, and different PEFT methods. To foster continuous progress in this
field, the code and all PEFT-weight plugins have been made open-source.
Title: LLAMA Millimeter and Submillimeter Observatory. Update on its Science
  Opportunities
Abstract: The Large Latin American Millimeter Array (LLAMA for short) is a joint
scientific and technological undertaking of Argentina and Brazil whose goal is
to install and to operate an observing facility capable of performing
observations of the Universe at millimeter and sub-millimeter wavelengths. It
will consist of a 12m ALMA-like antenna with the addition of two Nasmyth
cabins. LLAMA is located at 4850m above sea level in the Puna Saltenia, in the
northwest region of Argentina. When completed, LLAMA will be equipped with six
ALMA receivers covering Bands 1, 2+3, 5, 6, 7, and 9, which will populate the
two Nasmyth cabins. We summarize here the main ideas related with the Science
that LLAMA could accomplish on different astronomical topics, gathered from the
experience of a group of international experts on each field.
Title: LLAMA: Leveraging Learning to Automatically Manage Algorithms
Abstract: Algorithm portfolio and selection approaches have achieved remarkable
improvements over single solvers. However, the implementation of such systems
is often highly customised and specific to the problem domain. This makes it
difficult for researchers to explore different techniques for their specific
problems. We present LLAMA, a modular and extensible toolkit implemented as an
R package that facilitates the exploration of a range of different portfolio
techniques on any problem domain. It implements the algorithm selection
approaches most commonly used in the literature and leverages the extensive
library of machine learning algorithms and techniques in R. We describe the
current capabilities and limitations of the toolkit and illustrate its usage on
a set of example SAT problems.
Title: QIGen: Generating Efficient Kernels for Quantized Inference on Large
  Language Models
Abstract: We present ongoing work on a new automatic code generation approach for
supporting quantized generative inference on LLMs such as LLaMA or OPT on
off-the-shelf CPUs. Our approach is informed by the target architecture and a
performance model, including both hardware characteristics and method-specific
accuracy constraints. Results on CPU-based inference for LLaMA models show that
our approach can lead to high performance and high accuracy, comparing
favorably to the best existing open-source solution. A preliminary
implementation is available at https://github.com/IST-DASLab/QIGen.
Title: Analysis of Disinformation and Fake News Detection Using Fine-Tuned
  Large Language Model
Abstract: The paper considers the possibility of fine-tuning Llama 2 large language
model (LLM) for the disinformation analysis and fake news detection. For
fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was
fine-tuned for the following tasks: analysing a text on revealing
disinformation and propaganda narratives, fact checking, fake news detection,
manipulation analytics, extracting named entities with their sentiments. The
obtained results show that the fine-tuned Llama 2 model can perform a deep
analysis of texts and reveal complex styles and narratives. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models.
Title: Transit time derivation for hot planet bow-shocks
Abstract: We present an analytical estimate of hot planet bow-shock transit times that
will be useful for planning observations of such signatures.
Title: Challenges and opportunities integrating LLAMA into AdePT
Abstract: Particle transport simulations are a cornerstone of high-energy physics
(HEP), constituting a substantial part of the computing workload performed in
HEP. To boost the simulation throughput and energy efficiency, GPUs as
accelerators have been explored in recent years, further driven by the
increasing use of GPUs on HPCs. The Accelerated demonstrator of electromagnetic
Particle Transport (AdePT) is an advanced prototype for offloading the
simulation of electromagnetic showers in Geant4 to GPUs, and still undergoes
continuous development and optimization. Improving memory layout and data
access is vital to use modern, massively parallel GPU hardware efficiently,
contributing to the challenge of migrating traditional CPU based data
structures to GPUs in AdePT. The low-level abstraction of memory access (LLAMA)
is a C++ library that provides a zero-runtime-overhead data structure
abstraction layer, focusing on multidimensional arrays of nested, structured
data. It provides a framework for defining and switching custom memory mappings
at compile time to define data layouts and instrument data access, making LLAMA
an ideal tool to tackle the memory-related optimization challenges in AdePT.
Our contribution shares insights gained with LLAMA when instrumenting data
access inside AdePT, complementing traditional GPU profiler outputs. We
demonstrate traces of read/write counts to data structure elements as well as
memory heatmaps. The acquired knowledge allowed for subsequent data layout
optimizations.
Title: Sheared LLaMA: Accelerating Language Model Pre-training via Structured
  Pruning
Abstract: The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged
moderate-sized large language models (LLMs) highlights the potential of
building smaller yet powerful LLMs. Regardless, the cost of training such
models from scratch on trillions of tokens remains high. In this work, we study
structured pruning as an effective means to develop smaller LLMs from
pre-trained, larger models. Our approach employs two key techniques: (1)
targeted structured pruning, which prunes a larger model to a specified target
shape by removing layers, heads, and intermediate and hidden dimensions in an
end-to-end manner, and (2) dynamic batch loading, which dynamically updates the
composition of sampled data in each training batch based on varying losses
across different domains. We demonstrate the efficacy of our approach by
presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B
and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art
open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA
models, on a wide range of downstream and instruction tuning evaluations, while
requiring only 3% of compute compared to training such models from scratch.
This work provides compelling evidence that leveraging existing LLMs with
structured pruning is a far more cost-effective approach for building smaller
LLMs.
Title: Updates on the Low-Level Abstraction of Memory Access
Abstract: Choosing the best memory layout for each hardware architecture is
increasingly important as more and more programs become memory bound. For
portable codes that run across heterogeneous hardware architectures, the choice
of the memory layout for data structures is ideally decoupled from the rest of
a program. The low-level abstraction of memory access (LLAMA) is a C++ library
that provides a zero-runtime-overhead abstraction layer, underneath which
memory mappings can be freely exchanged to customize data layouts, memory
access and access instrumentation, focusing on multidimensional arrays of
nested, structured data.
  After its scientific debut, several improvements and extensions have been
added to LLAMA. This includes compile-time array extents for
zero-memory-overhead views, support for computations during memory access, new
mappings for bit-packing, switching types, byte-splitting, memory access
instrumentation, and explicit SIMD support. This contribution provides an
overview of recent developments in the LLAMA library.
Title: Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks
Abstract: We introduce Goat, a fine-tuned LLaMA model that significantly outperforms
GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated
dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic
sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the
accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve
near-perfect accuracy on large-number addition and subtraction through
supervised fine-tuning only, which is almost impossible with previous
pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute
Goat's exceptional performance to LLaMA's consistent tokenization of numbers.
To tackle more challenging tasks like large-number multiplication and division,
we propose an approach that classifies tasks based on their learnability, and
subsequently decomposes unlearnable tasks, such as multi-digit multiplication
and division, into a series of learnable tasks by leveraging basic arithmetic
principles. We thoroughly examine the performance of our model, offering a
comprehensive evaluation of the effectiveness of our proposed decomposition
steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM
GPU, facilitating reproducibility for other researchers. We release our model,
dataset, and the Python script for dataset generation.
Title: Lawyer LLaMA Technical Report
Abstract: Large Language Models (LLMs), like LLaMA, have exhibited remarkable
performance across various tasks. Nevertheless, when deployed to specific
domains such as law or medicine, the models still confront the challenge of a
deficiency in domain-specific knowledge and an inadequate capability to
leverage that knowledge to resolve domain-related problems. In this paper, we
propose a new framework to adapt LLMs to specific domains and build Lawyer
LLaMA, a legal domain LLM, based on this framework. Specifically, we inject
domain knowledge during the continual training stage and teach the model to
learn professional skills using properly designed supervised fine-tuning tasks.
Moreover, to alleviate the hallucination problem during the model's generation,
we add a retrieval module and extract relevant legal articles before the model
answers any queries. When learning domain-specific skills, we find that
experts' experience is much more useful than experiences distilled from
ChatGPT, where hundreds of expert-written data outperform tens of thousands of
ChatGPT-generated ones. We will release our model and data.
Title: Local Large Language Models for Complex Structured Medical Tasks
Abstract: This paper introduces an approach that combines the language reasoning
capabilities of large language models (LLMs) with the benefits of local
training to tackle complex, domain-specific tasks. Specifically, the authors
demonstrate their approach by extracting structured condition codes from
pathology reports. The proposed approach utilizes local LLMs, which can be
fine-tuned to respond to specific generative instructions and provide
structured outputs. The authors collected a dataset of over 150k uncurated
surgical pathology reports, containing gross descriptions, final diagnoses, and
condition codes. They trained different model architectures, including LLaMA,
BERT and LongFormer and evaluated their performance. The results show that the
LLaMA-based models significantly outperform BERT-style models across all
evaluated metrics, even with extremely reduced precision. The LLaMA models
performed especially well with large datasets, demonstrating their ability to
handle complex, multi-label tasks. Overall, this work presents an effective
approach for utilizing LLMs to perform domain-specific tasks using accessible
hardware, with potential applications in the medical domain, where complex data
extraction and classification are required.
Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model
Abstract: The paper considers the possibility to fine-tune Llama 2 GPT large language
model (LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.
Title: Gender-specific Machine Translation with Large Language Models
Abstract: Decoder-only Large Language Models (LLMs) have demonstrated potential in
machine translation (MT), albeit with performance slightly lagging behind
traditional encoder-decoder Neural Machine Translation (NMT) systems. However,
LLMs offer a unique advantage: the ability to control the properties of the
output through prompts. In this study, we harness this flexibility to explore
LLaMa's capability to produce gender-specific translations for languages with
grammatical gender. Our results indicate that LLaMa can generate
gender-specific translations with competitive accuracy and gender bias
mitigation when compared to NLLB, a state-of-the-art multilingual NMT system.
Furthermore, our experiments reveal that LLaMa's translations are robust,
showing significant performance drops when evaluated against opposite-gender
references in gender-ambiguous datasets but maintaining consistency in less
ambiguous contexts. This research provides insights into the potential and
challenges of using LLMs for gender-specific translations and highlights the
importance of in-context learning to elicit new tasks in LLMs.
Title: Contrastive Decoding Improves Reasoning in Large Language Models
Abstract: We demonstrate that Contrastive Decoding -- a simple, computationally light,
and training-free text generation method proposed by Li et al 2022 -- achieves
large out-of-the-box improvements over greedy decoding on a variety of
reasoning tasks. Originally shown to improve the perceived quality of long-form
text generation, Contrastive Decoding searches for strings that maximize a
weighted difference in likelihood between strong and weak models. We show that
Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM
2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA
2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in
addition to improvements on a collection of other tasks. Analysis suggests that
Contrastive Decoding improves over existing methods by preventing some abstract
reasoning errors, as well as by avoiding simpler modes such as copying sections
of the input during chain-of-thought. Overall, Contrastive Decoding outperforms
nucleus sampling for long-form generation and greedy decoding for reasoning
tasks, making it a powerful general purpose method for generating text from
language models.
Title: AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with
  TikZ
Abstract: Generating bitmap graphics from text has gained considerable attention, yet
for scientific figures, vector graphics are often preferred. Given that vector
graphics are typically encoded using low-level graphics primitives, generating
them directly is difficult. To address this, we propose the use of TikZ, a
well-known abstract graphics language that can be compiled to vector graphics,
as an intermediate representation of scientific figures. TikZ offers
human-oriented, high-level commands, thereby facilitating conditional language
modeling with any large language model. To this end, we introduce DaTikZ the
first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with
captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which
augments LLaMA with multimodal CLIP embeddings. In both human and automatic
evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms
of similarity to human-created figures, with CLiMA additionally improving
text-image alignment. Our detailed analysis shows that all models generalize
well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend
to generate more simplistic figures compared to both humans and our models. We
make our framework, AutomaTikZ, along with model weights and datasets, publicly
available.
Title: Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on
  Open-Source Model
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
utilizing tools, but their closed-source nature and high inference costs pose
limitations on their adaptability, necessitating a valid method that leverages
smaller, open-sourced models. In this paper, we introduce Toolink, a
comprehensive framework that performs task-solving by first creating a toolkit
and then integrating the planning and calling of tools through a
chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in
harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we
curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and
finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source
model with advanced tool-planning and tool-calling capabilities. Evaluation on
diverse tasks from BIG-bench demonstrates its CoS ability matches that of
ChatGPT while its performance surpasses the chain-of-thought approach. Further
studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase
its capability in using toolkits not explicitly tailored for the target task,
affirming its robustness in real-world scenarios. All codes and data are
released.
Title: Whispering LLaMA: A Cross-Modal Generative Error Correction Framework
  for Speech Recognition
Abstract: We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
Title: Mistral 7B
Abstract: We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered
for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B
across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and
code generation. Our model leverages grouped-query attention (GQA) for faster
inference, coupled with sliding window attention (SWA) to effectively handle
sequences of arbitrary length with a reduced inference cost. We also provide a
model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses
the Llama 2 13B -- Chat model both on human and automated benchmarks. Our
models are released under the Apache 2.0 license.
Title: An Open Source Data Contamination Report for Large Language Models
Abstract: Data contamination in language model evaluation is increasingly prevalent as
the popularity of large language models. It allows models to "cheat" via
memorisation instead of displaying true capabilities. Therefore, contamination
analysis has became an crucial part of reliable model evaluation to validate
results. However, existing contamination analysis is usually conducted
internally by LLM developers and often lacks transparency and completeness.
This paper present an open source data contamination reports for the Llama
series models. We analyse six popular multi-choice QA benchmarks and quantify
their overlapping with the training set of Llama. Various levels of
contamination ranging from 1\% to 8.7\% are found across benchmarks. Our
comparison also reveals that Llama models can gain over 5\% higher accuracy on
contaminated subsets versus clean subsets. Data and code are available at:
https://github.com/liyucheng09/Contamination_Detector.
Title: Impact of Tokenization on LLaMa Russian Adaptation
Abstract: Latest instruction-tuned large language models (LLM) show great results on
various tasks, however, they often face performance degradation for non-English
input. There is evidence that the reason lies in inefficient tokenization
caused by low language representation in pre-training data which hinders the
comprehension of non-English instructions, limiting the potential of target
language instruction-tuning. In this work we investigate the possibility of
addressing the issue with vocabulary substitution in the context of LLaMa
Russian language adaptation. We explore three variants of vocabulary adaptation
and test their performance on Saiga instruction-tuning and fine-tuning on
Russian Super Glue benchmark. The results of automatic evaluation show that
vocabulary substitution not only improves the model's quality in Russian but
also accelerates fine-tuning (35%) and inference (up to 60%) while reducing
memory consumption. Additional human evaluation of the instruction-tuned models
demonstrates that models with Russian-adapted vocabulary generate answers with
higher user preference than the original Saiga-LLaMa model.
Title: Label Supervised LLaMA Finetuning
Abstract: The recent success of Large Language Models (LLMs) has gained significant
attention in both academia and industry. Substantial efforts have been made to
enhance the zero- and few-shot generalization capabilities of open-source LLMs
through finetuning. Currently, the prevailing approach is instruction-tuning,
which trains LLMs to complete real-world tasks by generating responses guided
by natural language instructions. It is worth noticing that such an approach
may underperform in sequence and token classification tasks. Unlike text
generation tasks, classification tasks have a limited label space, where
precise label prediction is more appreciated than generating diverse and
human-like responses. Prior research has unveiled that instruction-tuned LLMs
cannot outperform BERT, prompting us to explore the potential of leveraging
latent representations from LLMs for supervised label prediction. In this
paper, we introduce a label-supervised adaptation for LLMs, which aims to
finetuning the model with discriminant labels. We evaluate this approach with
Label Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively
small-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We
extract latent representations from the final LLaMA layer and project them into
the label space to compute the cross-entropy loss. The model is finetuned by
Low-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate
prompt engineering or external knowledge, LS-LLaMA substantially outperforms
LLMs ten times its size in scale and demonstrates consistent improvements
compared to robust baselines like BERT-Large and RoBERTa-Large in text
classification. Moreover, by removing the causal mask from decoders, LS-unLLaMA
achieves the state-of-the-art performance in named entity recognition (NER).
Our work will shed light on a novel approach to adapting LLMs for various
downstream tasks.
Title: Giraffe: Adventures in Expanding Context Lengths in LLMs
Abstract: Modern large language models (LLMs) that rely on attention mechanisms are
typically trained with fixed context lengths which enforce upper limits on the
length of input sequences that they can handle at evaluation time. To use these
models on sequences longer than the train-time context length, one might employ
techniques from the growing family of context length extrapolation methods --
most of which focus on modifying the system of positional encodings used in the
attention mechanism to indicate where tokens or activations are located in the
input sequence. We conduct a wide survey of existing methods of context length
extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own
design as well -- in particular, a new truncation strategy for modifying the
basis for the position encoding.
  We test these methods using three new evaluation tasks (FreeFormQA,
AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to
be less fine-grained as a measure of long context performance of LLMs. We
release the three tasks publicly as datasets on HuggingFace. We discover that
linear scaling is the best method for extending context length, and show that
further gains can be achieved by using longer scales at evaluation time. We
also discover promising extrapolation capabilities in the truncated basis. To
support further research in this area, we release three new 13B parameter
long-context models which we call Giraffe: 4k and 16k context models trained
from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We
also release the code to replicate our results.
Title: LitCab: Lightweight Calibration of Language Models on Outputs of Varied
  Lengths
Abstract: A model is considered well-calibrated when its probability estimate aligns
with the actual likelihood of the output being correct. Calibrating language
models (LMs) is crucial, as it plays a vital role in detecting and mitigating
hallucinations, a common issue of LMs, as well as building more trustworthy
models. Yet, popular neural model calibration techniques are not well-suited
for LMs due to their lack of flexibility in discerning answer correctness and
their high computational costs. For instance, post-processing methods like
temperature scaling are often unable to reorder the candidate generations.
Moreover, training-based methods require finetuning the entire model, which is
impractical due to the increasing sizes of modern LMs. In this paper, we
present LitCab, a lightweight calibration mechanism consisting of a single
linear layer taking the input text representation and manipulateing the LM
output logits. LitCab improves model calibration by only adding < 2% of the
original model parameters. For evaluation, we construct CaT, a benchmark
consisting of 7 text generation tasks, covering responses ranging from short
phrases to paragraphs. We test LitCab with Llama2-7B, where it improves
calibration across all tasks, by reducing the average ECE score by 20%. We
further conduct a comprehensive evaluation with 7 popular open-sourced LMs from
GPT and LLaMA families, yielding the following key findings: (1) Larger models
within the same family exhibit better calibration on tasks with short
generation tasks, but not necessarily for longer ones. (2) GPT-family models
show superior calibration compared to LLaMA, Llama2 and Vicuna models despite
having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA)
with samples of limited purpose (e.g., conversations) may lead to worse
calibration, highlighting the importance of finetuning setups for calibrating
LMs.
Title: BioInstruct: Instruction Tuning of Large Language Models for Biomedical
  Natural Language Processing
Abstract: To enhance the performance of large language models (LLMs) in biomedical
natural language processing (BioNLP) by introducing a domain-specific
instruction dataset and examining its impact when combined with multi-task
learning principles. We created the BioInstruct, comprising 25,005 instructions
to instruction-tune LLMs(LLaMA 1 & 2, 7B & 13B version). The instructions were
created by prompting the GPT-4 language model with three-seed samples randomly
drawn from an 80 human curated instructions. We employed Low-Rank
Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these
instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three
major categories: question answering(QA), information extraction(IE), and text
generation(GEN). We also examined whether categories(e.g., QA, IE, and
generation) of instructions impact model performance. Comparing with LLMs
without instruction-tuned, our instruction-tuned LLMs demonstrated marked
performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our
7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed
other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with
vast domain-specific data or a variety of tasks. Our results also show that the
performance gain is significantly higher when instruction fine-tuning is
conducted with closely related tasks. Our findings align with the observations
of multi-task learning, suggesting the synergies between two tasks. The
BioInstruct dataset serves as a valuable resource and instruction tuned LLMs
lead to the best performing BioNLP applications.
Title: MAP's not dead yet: Uncovering true language model modes by conditioning
  away degeneracy
Abstract: It has been widely observed that exact or approximate MAP (mode-seeking)
decoding from natural language generation (NLG) models consistently leads to
degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has
generally been attributed to either a fundamental inadequacy of modes in models
or weaknesses in language modeling. Contrastingly in this work, we emphasize
that degenerate modes can even occur in the absence of any model error, due to
contamination of the training data. Specifically, we show that mixing even a
tiny amount of low-entropy noise with a population text distribution can cause
the data distribution's mode to become degenerate, implying that any models
trained on it will be as well. As the unconditional mode of NLG models will
often be degenerate, we therefore propose to apply MAP decoding to the model's
distribution conditional on avoiding specific degeneracies. Using exact-search,
we empirically verify that the length-conditional modes of machine translation
models and language models are indeed more fluent and topical than their
unconditional modes. For the first time, we also share many examples of exact
modal sequences from these models, and from several variants of the LLaMA-7B
model. Notably, the modes of the LLaMA models are still degenerate, showing
that improvements in modeling have not fixed this issue. Because of the cost of
exact mode finding algorithms, we develop an approximate mode finding approach,
ACBS, which finds sequences that are both high-likelihood and high-quality. We
apply this approach to LLaMA-7B, a model which was not trained for instruction
following, and find that we are able to elicit reasonable outputs without any
finetuning.
Title: Assessing Translation capabilities of Large Language Models involving
  English and Indian Languages
Abstract: Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. In this work, our aim is to explore the multilingual
capabilities of large language models by using machine translation as a task
involving English and 22 Indian languages. We first investigate the translation
capabilities of raw large language models, followed by exploring the in-context
learning capabilities of the same raw models. We fine-tune these large language
models using parameter efficient fine-tuning methods such as LoRA and
additionally with full fine-tuning. Through our study, we have identified the
best performing large language model for the translation task involving LLMs,
which is based on LLaMA.
  Our results demonstrate significant progress, with average BLEU scores of
13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,
42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for
English to Indian languages on IN22 (conversational), IN22 (general),
flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for
Indian languages to English, we achieved average BLEU scores of 14.03, 16.65,
16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,
and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),
IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.
Overall, our findings highlight the potential and strength of large language
models for machine translation capabilities, including for languages that are
currently underrepresented in LLMs.
Title: Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2
Abstract: Since the release of T\"ULU [Wang et al., 2023b], open resources for
instruction tuning have developed quickly, from better base models to new
finetuning techniques. We test and incorporate a number of these advances into
T\"ULU, resulting in T\"ULU 2, a suite of improved T\"ULU models for advancing
the understanding and best practices of adapting pretrained language models to
downstream tasks and user preferences. Concretely, we release: (1)
T\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2)
T\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\"ULU 2+DPO, T\"ULU
2 models trained with direct preference optimization (DPO), including the
largest DPO-trained model to date (T\"ULU 2+DPO 70B); (4) CODE T\"ULU 2, CODE
LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its
instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple
perspectives shows that the T\"ULU 2 suite achieves state-of-the-art
performance among open models and matches or exceeds the performance of
GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data,
training and evaluation code to facilitate future open efforts on adapting
large language models.
Title: Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models
Abstract: This paper presents CyberSecEval, a comprehensive benchmark developed to help
bolster the cybersecurity of Large Language Models (LLMs) employed as coding
assistants. As what we believe to be the most extensive unified cybersecurity
safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs
in two crucial security domains: their propensity to generate insecure code and
their level of compliance when asked to assist in cyberattacks. Through a case
study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large
language model families, CyberSecEval effectively pinpointed key cybersecurity
risks. More importantly, it offered practical insights for refining these
models. A significant observation from the study was the tendency of more
advanced models to suggest insecure code, highlighting the critical need for
integrating security considerations in the development of sophisticated LLMs.
CyberSecEval, with its automated test case generation and evaluation pipeline
covers a broad scope and equips LLM designers and researchers with a tool to
broadly measure and enhance the cybersecurity safety properties of LLMs,
contributing to the development of more secure AI systems.
Title: Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens
Abstract: Recent advances in large video-language models have displayed promising
outcomes in video comprehension. Current approaches straightforwardly convert
video into language tokens and employ large language models for multi-modal
tasks. However, this method often leads to the generation of irrelevant
content, commonly known as "hallucination", as the length of the text increases
and the impact of the video diminishes. To address this problem, we propose
Vista-LLaMA, a novel framework that maintains the consistent distance between
all visual tokens and any language tokens, irrespective of the generated text
length. Vista-LLaMA omits relative position encoding when determining attention
weights between visual and text tokens, retaining the position encoding for
text and text tokens. This amplifies the effect of visual tokens on text
generation, especially when the relative distance is longer between visual and
text tokens. The proposed attention mechanism significantly reduces the chance
of producing irrelevant text related to the video content. Furthermore, we
present a sequential visual projector that projects the current video frame
into tokens of language space with the assistance of the previous frame. This
approach not only captures the temporal relationship within the video, but also
allows less visual tokens to encompass the entire video. Our approach
significantly outperforms various previous methods (e.g., Video-ChatGPT,
MovieChat) on four challenging open-ended video question answering benchmarks.
We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot
MSRVTT-QA, setting a new state-of-the-art performance. This project is
available at https://jinxxian.github.io/Vista-LLaMA.
Title: LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian
  Language
Abstract: Large Language Models represent state-of-the-art linguistic models designed
to equip computers with the ability to comprehend natural language. With its
exceptional capacity to capture complex contextual relationships, the LLaMA
(Large Language Model Meta AI) family represents a novel advancement in the
field of natural language processing by releasing foundational models designed
to improve the natural language understanding abilities of the transformer
architecture thanks to their large amount of trainable parameters (7, 13, and
70 billion parameters). In many natural language understanding tasks, these
models obtain the same performances as private company models such as OpenAI
Chat-GPT with the advantage to make publicly available weights and code for
research and commercial uses. In this work, we investigate the possibility of
Language Adaptation for LLaMA models, explicitly focusing on addressing the
challenge of Italian Language coverage. Adopting an open science approach, we
explore various tuning approaches to ensure a high-quality text generated in
Italian suitable for common tasks in this underrepresented language in the
original models' datasets. We aim to release effective text generation models
with strong linguistic properties for many tasks that seem challenging using
multilingual or general-purpose LLMs. By leveraging an open science philosophy,
this study contributes to Language Adaptation strategies for the Italian
language by introducing the novel LLaMAntino family of Italian LLMs.
Title: MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
Abstract: Recent advancements in Text-to-SQL methods employing Large Language Models
(LLMs) have demonstrated remarkable performance. Nonetheless, these approaches
continue to encounter difficulties when handling extensive databases, intricate
user queries, and erroneous SQL results. To tackle these challenges, we present
\textsc{MAC-SQL}, a novel LLM-based multi-agent collaborative framework
designed for the Text-to-SQL task. Our framework comprises three agents: the
\textit{Selector}, accountable for condensing voluminous databases and
preserving relevant table schemas for user questions; the \textit{Decomposer},
which disassembles complex user questions into more straightforward
sub-problems and resolves them progressively; and the \textit{Refiner}, tasked
with validating and refining defective SQL queries. We perform comprehensive
experiments on two Text-to-SQL datasets, BIRD and Spider, achieving a
state-of-the-art execution accuracy of 59.59\% on the BIRD test set. Moreover,
we have open-sourced an instruction fine-tuning model, SQL-Llama, based on Code
Llama 7B, in addition to an agent instruction dataset derived from training
data based on BIRD and Spider. The SQL-Llama model has demonstrated encouraging
results on the development sets of both BIRD and Spider. However, when compared
to GPT-4, there remains a notable potential for enhancement. Our code and data
are publicly available at https://github.com/wbbeyourself/MAC-SQL.
Title: LLaMA Beyond English: An Empirical Study on Language Capability Transfer
Abstract: In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model's level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model's response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
Title: Large Latin American Millimeter Array
Abstract: The Large Latin American Millimeter Array (LLAMA) is a multipurpose
single-dish 12 m radiotelescope with VLBI capability under construction in the
Puna de Atacama desert in the Province of Salta, Argentina. In this paper I
review the project, the instrument, the current status, and the scientific
goals of this astronomical collaboration between Argentina and Brazil.
Title: ImageBind-LLM: Multi-modality Instruction Tuning
Abstract: We present ImageBind-LLM, a multi-modality instruction tuning method of large
language models (LLMs) via ImageBind. Existing works mainly focus on language
and image instruction tuning, different from which, our ImageBind-LLM can
respond to multi-modality conditions, including audio, 3D point clouds, video,
and their embedding-space arithmetic by only image-text alignment training.
During training, we adopt a learnable bind network to align the embedding space
between LLaMA and ImageBind's image encoder. Then, the image features
transformed by the bind network are added to word tokens of all layers in
LLaMA, which progressively injects visual instructions via an attention-free
and zero-initialized gating mechanism. Aided by the joint embedding of
ImageBind, the simple image-text training enables our model to exhibit superior
multi-modality instruction-following capabilities. During inference, the
multi-modality inputs are fed into the corresponding ImageBind encoders, and
processed by a proposed visual cache model for further cross-modal embedding
enhancement. The training-free cache model retrieves from three million image
features extracted by ImageBind, which effectively mitigates the
training-inference modality discrepancy. Notably, with our approach,
ImageBind-LLM can respond to instructions of diverse modalities and demonstrate
significant language generation quality. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
Title: K2 reveals pulsed accretion driven by the 2 Myr old hot Jupiter CI Tau b
Abstract: CI Tau is a young (~2 Myr) classical T Tauri star located in the Taurus star
forming region. Radial velocity observations indicate it hosts a Jupiter-sized
planet with an orbital period of approximately 9 days. In this work, we analyze
time series of CI Tau's photometric variability as seen by K2. The lightcurve
reveals the stellar rotation period to be ~6.6 d. Although there is no evidence
that CI Tau b transits the host star, a ~9 d signature is also present in the
lightcurve. We believe this is most likely caused by planet-disk interactions
which perturb the accretion flow onto the star, resulting in a periodic
modulation of the brightness with the ~9 d period of the planet's orbit.
Title: Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in
  Foundation Models
Abstract: In this work, we assess the ability of foundation models to recall
encyclopedic knowledge across a wide range of linguistic contexts. To support
this, we: 1) produce a 20-language dataset that contains 303k factual
associations paired with counterfactuals, 2) evaluate 5 models in a
multilingual test, and 3) benchmark a diverse set of 24 models in an
English-only test. Meta's LLaMA achieves the highest scores in both
multilingual and English-only evaluations. Yet, an analysis of LLaMA's errors
reveals significant limitations in its ability to recall facts in languages
other than English, plus difficulties related to the location and gender of
fact subjects. Overall, our findings suggest that today's foundation models are
far from polyglots.
Title: Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
Abstract: Language models (LMs) often struggle to pay enough attention to the input
context, and generate texts that are unfaithful or contain hallucinations. To
mitigate this issue, we present context-aware decoding (CAD), which follows a
contrastive output distribution that amplifies the difference between the
output probabilities when a model is used with and without context. Our
experiments show that CAD, without additional training, significantly improves
the faithfulness of different LM families, including OPT, GPT, LLaMA and
FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality
metrics). Furthermore, CAD is particularly effective in overriding a model's
prior knowledge when it contradicts the provided context, leading to
substantial improvements in tasks where resolving the knowledge conflict is
essential.
Title: Inference-Time Intervention: Eliciting Truthful Answers from a Language
  Model
Abstract: We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the "truthfulness" of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
Title: Extending Context Window of Large Language Models via Positional
  Interpolation
Abstract: We present Position Interpolation (PI) that extends the context window sizes
of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal
fine-tuning (within 1000 steps), while demonstrating strong empirical results
on various tasks that require long context, including passkey retrieval,
language modeling, and long document summarization from LLaMA 7B to 65B.
Meanwhile, the extended model by Position Interpolation preserve quality
relatively well on tasks within its original context window. To achieve this
goal, Position Interpolation linearly down-scales the input position indices to
match the original context window size, rather than extrapolating beyond the
trained context length which may lead to catastrophically high attention scores
that completely ruin the self-attention mechanism. Our theoretical study shows
that the upper bound of interpolation is at least $\sim 600 \times$ smaller
than that of extrapolation, further demonstrating its stability. Models
extended via Position Interpolation retain its original architecture and can
reuse most pre-existing optimization and infrastructure.
Title: Prompting Large Language Models for Zero-Shot Domain Adaptation in
  Speech Recognition
Abstract: The integration of Language Models (LMs) has proven to be an effective way to
address domain shifts in speech recognition. However, these approaches usually
require a significant amount of target domain text data for the training of
LMs. Different from these methods, in this work, with only a domain-specific
text prompt, we propose two zero-shot ASR domain adaptation methods using
LLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two
ways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR
system with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an
encoder-decoder based ASR system. Experiments show that, with only one domain
prompt, both methods can effectively reduce word error rates (WER) on
out-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep
LLM-fusion has the advantage of better recall of entity and out-of-vocabulary
words.
Title: Stay on topic with Classifier-Free Guidance
Abstract: Classifier-Free Guidance (CFG) has recently emerged in text-to-image
generation as a lightweight technique to encourage prompt-adherence in
generations. In this work, we demonstrate that CFG can be used broadly as an
inference-time technique in pure language modeling. We show that CFG (1)
improves the performance of Pythia, GPT-2 and LLaMA-family models across an
array of tasks: Q\&A, reasoning, code generation, and machine translation,
achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements
equivalent to a model with twice the parameter-count; (3) can stack alongside
other inference-time methods like Chain-of-Thought and Self-Consistency,
yielding further improvements in difficult tasks; (4) can be used to increase
the faithfulness and coherence of assistants in challenging form-driven and
content-driven prompts: in a human evaluation we show a 75\% preference for
GPT4All using CFG over baseline.
Title: SCITUNE: Aligning Large Language Models with Scientific Multimodal
  Instructions
Abstract: Instruction finetuning is a popular paradigm to align large language models
(LLM) with human intent. Despite its popularity, this idea is less explored in
improving the LLMs to align existing foundation models with scientific
disciplines, concepts and goals. In this work, we present SciTune as a tuning
framework to improve the ability of LLMs to follow scientific multimodal
instructions. To test our methodology, we use a human-generated scientific
instruction tuning dataset and train a large multimodal model LLaMA-SciTune
that connects a vision encoder and LLM for science-focused visual and language
understanding. In comparison to the models that are finetuned with machine
generated data only, LLaMA-SciTune surpasses human performance on average and
in many sub-categories on the ScienceQA benchmark.
Title: Camoscio: an Italian Instruction-tuned LLaMA
Abstract: In recent years Large Language Models (LLMs) have increased the state of the
art on several natural language processing tasks. However, their accessibility
is often limited to paid API services, posing challenges for researchers in
conducting extensive investigations. On the other hand, while some open-source
models have been proposed by the community, they are typically English-centric
or multilingual without a specific adaptation for the Italian language. In an
effort to democratize the available and open resources for the Italian
language, in this paper we introduce Camoscio: a language model specifically
tuned to follow users' prompts in Italian. Specifically, we finetuned the
smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts
translated to Italian via ChatGPT. Results indicate that the model's zero-shot
performance on various downstream tasks in Italian competes favorably with
existing models specifically finetuned for those tasks. All the artifacts
(code, dataset, model) are released to the community at the following url:
https://github.com/teelinsan/camoscio
Title: Self-Alignment with Instruction Backtranslation
Abstract: We present a scalable method to build a high quality instruction following
language model by automatically labelling human-written text with corresponding
instructions. Our approach, named instruction backtranslation, starts with a
language model finetuned on a small amount of seed data, and a given web
corpus. The seed model is used to construct training examples by generating
instruction prompts for web documents (self-augmentation), and then selecting
high quality examples from among these candidates (self-curation). This data is
then used to finetune a stronger model. Finetuning LLaMa on two iterations of
our approach yields a model that outperforms all other LLaMa-based models on
the Alpaca leaderboard not relying on distillation data, demonstrating highly
effective self-alignment.
Title: Three Ways of Using Large Language Models to Evaluate Chat
Abstract: This paper describes the systems submitted by team6 for ChatEval, the DSTC 11
Track 4 competition. We present three different approaches to predicting
turn-level qualities of chatbot responses based on large language models
(LLMs). We report improvement over the baseline using dynamic few-shot examples
from a vector store for the prompts for ChatGPT. We also analyze the
performance of the other two approaches and report needed improvements for
future work. We developed the three systems over just two weeks, showing the
potential of LLMs for this task. An ablation study conducted after the
challenge deadline shows that the new Llama 2 models are closing the
performance gap between ChatGPT and open-source LLMs. However, we find that the
Llama 2 models do not benefit from few-shot examples in the same way as
ChatGPT.
Title: LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked
Abstract: Large language models (LLMs) are popular for high-quality text generation but
can produce harmful content, even when aligned with human values through
reinforcement learning. Adversarial prompts can bypass their safety measures.
We propose LLM Self Defense, a simple approach to defend against these attacks
by having an LLM screen the induced responses. Our method does not require any
fine-tuning, input preprocessing, or iterative output generation. Instead, we
incorporate the generated content into a pre-defined prompt and employ another
instance of an LLM to analyze the text and predict whether it is harmful. We
test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent
LLMs against various types of attacks, such as forcefully inducing affirmative
responses to prompts and prompt engineering attacks. Notably, LLM Self Defense
succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5
and Llama 2.
Title: SoTaNa: The Open-Source Software Development Assistant
Abstract: Software development plays a crucial role in driving innovation and
efficiency across modern societies. To meet the demands of this dynamic field,
there is a growing need for an effective software development assistant.
However, existing large language models represented by ChatGPT suffer from
limited accessibility, including training data and model weights. Although
other large open-source models like LLaMA have shown promise, they still
struggle with understanding human intent. In this paper, we present SoTaNa, an
open-source software development assistant. SoTaNa utilizes ChatGPT to generate
high-quality instruction-based data for the domain of software engineering and
employs a parameter-efficient fine-tuning approach to enhance the open-source
foundation model, LLaMA. We evaluate the effectiveness of \our{} in answering
Stack Overflow questions and demonstrate its capabilities. Additionally, we
discuss its capabilities in code summarization and generation, as well as the
impact of varying the volume of generated data on model performance. Notably,
SoTaNa can run on a single GPU, making it accessible to a broader range of
researchers. Our code, model weights, and data are public at
\url{https://github.com/DeepSoftwareAnalytics/SoTaNa}.
Title: The Moral Machine Experiment on Large Language Models
Abstract: As large language models (LLMs) become more deeply integrated into various
sectors, understanding how they make moral judgments has become crucial,
particularly in the realm of autonomous driving. This study utilized the Moral
Machine framework to investigate the ethical decision-making tendencies of
prominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their
responses to human preferences. While LLMs' and humans' preferences such as
prioritizing humans over pets and favoring saving more lives are broadly
aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.
Additionally, despite the qualitative similarities between the LLM and human
preferences, there are significant quantitative disparities, suggesting that
LLMs might lean toward more uncompromising decisions, compared to the milder
inclinations of humans. These insights elucidate the ethical frameworks of LLMs
and their potential implications for autonomous driving.
Title: Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language
  Models that Follow Instructions
Abstract: Training large language models to follow instructions makes them perform
better on a wide range of tasks, generally becoming more helpful. However, a
perfectly helpful model will follow even the most malicious instructions and
readily generate harmful content. In this paper, we raise concerns over the
safety of models that only emphasize helpfulness, not safety, in their
instruction-tuning. We show that several popular instruction-tuned models are
highly unsafe. Moreover, we show that adding just 3% safety examples (a few
hundred demonstrations) in the training set when fine-tuning a model like LLaMA
can substantially improve their safety. Our safety-tuning does not make models
significantly less capable or helpful as measured by standard benchmarks.
However, we do find a behavior of exaggerated safety, where too much
safety-tuning makes models refuse to respond to reasonable prompts that
superficially resemble unsafe ones. Our study sheds light on trade-offs in
training LLMs to follow instructions and exhibit safe behavior.
